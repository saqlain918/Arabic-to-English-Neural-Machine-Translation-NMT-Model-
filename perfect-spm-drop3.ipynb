{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":10863294,"datasetId":6748615,"databundleVersionId":11228134},{"sourceType":"datasetVersion","sourceId":10882931,"datasetId":6762469,"databundleVersionId":11249929},{"sourceType":"datasetVersion","sourceId":10892207,"datasetId":6768897,"databundleVersionId":11260216},{"sourceType":"datasetVersion","sourceId":10888111,"datasetId":6765821,"databundleVersionId":11255660}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tokenizers datasets\n!pip install sentencepiece\nimport sentencepiece as spm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-01T23:22:23.580578Z","iopub.execute_input":"2025-03-01T23:22:23.580865Z","iopub.status.idle":"2025-03-01T23:22:31.176091Z","shell.execute_reply.started":"2025-03-01T23:22:23.580842Z","shell.execute_reply":"2025-03-01T23:22:31.175289Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.21.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.29.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.17.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from datasets import load_dataset\nfrom tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T23:22:31.177114Z","iopub.execute_input":"2025-03-01T23:22:31.177362Z","iopub.status.idle":"2025-03-01T23:22:33.056406Z","shell.execute_reply.started":"2025-03-01T23:22:31.177340Z","shell.execute_reply":"2025-03-01T23:22:33.055576Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\n\n# Load dataset from the Kaggle input directory, specifying the separator as '\\t' for a TSV file\ndata = pd.read_csv('/kaggle/input/dataseet/tatoeba-dev.ara-eng.tsv', sep='\\t', encoding='utf-8', header=None, names=['Arabic', 'English'])\n\n# Inspect the first few rows\nprint(data.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T23:22:33.057891Z","iopub.execute_input":"2025-03-01T23:22:33.058492Z","iopub.status.idle":"2025-03-01T23:22:33.165166Z","shell.execute_reply.started":"2025-03-01T23:22:33.058466Z","shell.execute_reply":"2025-03-01T23:22:33.164200Z"}},"outputs":[{"name":"stdout","text":"                                                    Arabic  \\\nacm eng  اذا الواحد يستخدم الفلوس بحمكة يكدر يسوي كومة ...   \n    eng                                 عمرك رايح المكسيك؟   \n    eng                       فكرنا انه طبيعي لازم يتعاقب.   \n    eng              لازم تترك الامور تاخذ مجراها الطبيعي.   \n    eng                                    لا يريدون استخ.   \n\n                                                   English  \nacm eng                 If wisely used, money can do much.  \n    eng                      Have you ever been to Mexico?  \n    eng  We thought that it was natural that he should ...  \n    eng         You must let things take their own course.  \n    eng                         They don't want to use it.  \n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"print(data.columns)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T23:22:33.166640Z","iopub.execute_input":"2025-03-01T23:22:33.166932Z","iopub.status.idle":"2025-03-01T23:22:33.171910Z","shell.execute_reply.started":"2025-03-01T23:22:33.166908Z","shell.execute_reply":"2025-03-01T23:22:33.170955Z"}},"outputs":[{"name":"stdout","text":"Index(['Arabic', 'English'], dtype='object')\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Drop rows with missing values\ndata_cleaned = data.dropna()\n\n# Inspect the cleaned dataset\nprint(data_cleaned.head())\nprint(\"Dataset Shape:\", data_cleaned.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T23:22:33.172698Z","iopub.execute_input":"2025-03-01T23:22:33.172944Z","iopub.status.idle":"2025-03-01T23:22:33.195518Z","shell.execute_reply.started":"2025-03-01T23:22:33.172923Z","shell.execute_reply":"2025-03-01T23:22:33.194755Z"}},"outputs":[{"name":"stdout","text":"                                                    Arabic  \\\nacm eng  اذا الواحد يستخدم الفلوس بحمكة يكدر يسوي كومة ...   \n    eng                                 عمرك رايح المكسيك؟   \n    eng                       فكرنا انه طبيعي لازم يتعاقب.   \n    eng              لازم تترك الامور تاخذ مجراها الطبيعي.   \n    eng                                    لا يريدون استخ.   \n\n                                                   English  \nacm eng                 If wisely used, money can do much.  \n    eng                      Have you ever been to Mexico?  \n    eng  We thought that it was natural that he should ...  \n    eng         You must let things take their own course.  \n    eng                         They don't want to use it.  \nDataset Shape: (19529, 2)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"data = data.drop_duplicates()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T23:22:33.196505Z","iopub.execute_input":"2025-03-01T23:22:33.196863Z","iopub.status.idle":"2025-03-01T23:22:33.225210Z","shell.execute_reply.started":"2025-03-01T23:22:33.196789Z","shell.execute_reply":"2025-03-01T23:22:33.224311Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import re\n\ndef remove_diacritics(text):\n    arabic_diacritics = re.compile(r'[\\u064B-\\u0652]')  # Match diacritics\n    text = re.sub(arabic_diacritics, '', text)  # Remove diacritics\n    text = text.replace(\"ى\", \"ي\").replace(\"ة\", \"ه\")  # Normalize letters\n    return text.strip()\n\ndata['Arabic'] = data['Arabic'].apply(remove_diacritics)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T23:22:33.226096Z","iopub.execute_input":"2025-03-01T23:22:33.226402Z","iopub.status.idle":"2025-03-01T23:22:33.284394Z","shell.execute_reply.started":"2025-03-01T23:22:33.226380Z","shell.execute_reply":"2025-03-01T23:22:33.283487Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"sample_text = \"كُتِبَ في الكِتابِ شيءٌ مُهِمٌّ.\"\nclean_text = remove_diacritics(sample_text)\nprint(\"Before:\", sample_text)\nprint(\"After :\", clean_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T23:22:33.286553Z","iopub.execute_input":"2025-03-01T23:22:33.286787Z","iopub.status.idle":"2025-03-01T23:22:33.292540Z","shell.execute_reply.started":"2025-03-01T23:22:33.286767Z","shell.execute_reply":"2025-03-01T23:22:33.291629Z"}},"outputs":[{"name":"stdout","text":"Before: كُتِبَ في الكِتابِ شيءٌ مُهِمٌّ.\nAfter : كتب في الكتاب شيء مهم.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"data['English'] = data['English'].str.lower()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T23:22:33.293642Z","iopub.execute_input":"2025-03-01T23:22:33.293940Z","iopub.status.idle":"2025-03-01T23:22:33.312218Z","shell.execute_reply.started":"2025-03-01T23:22:33.293919Z","shell.execute_reply":"2025-03-01T23:22:33.311226Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"data.to_csv(\"cleaned_dataset.tsv\", sep='\\t', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T23:22:33.313111Z","iopub.execute_input":"2025-03-01T23:22:33.313482Z","iopub.status.idle":"2025-03-01T23:22:33.391129Z","shell.execute_reply.started":"2025-03-01T23:22:33.313455Z","shell.execute_reply":"2025-03-01T23:22:33.390237Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def write_sentences_to_file(sentences, file_path):\n    with open(file_path, 'w', encoding='utf-8') as f:\n        for sentence in sentences:\n            f.write(sentence.strip() + '\\n')\n\n# Save Arabic and English sentences to files\nwrite_sentences_to_file(data_cleaned['Arabic'], 'arabic_sentences.txt')\nwrite_sentences_to_file(data_cleaned['English'], 'english_sentences.txt')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T23:22:33.391936Z","iopub.execute_input":"2025-03-01T23:22:33.392229Z","iopub.status.idle":"2025-03-01T23:22:33.418247Z","shell.execute_reply.started":"2025-03-01T23:22:33.392207Z","shell.execute_reply":"2025-03-01T23:22:33.417364Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import sentencepiece as spm\n\n# Train Arabic SentencePiece model\nspm.SentencePieceTrainer.train(\n    input='arabic_sentences.txt',\n    model_prefix='spm_arabic',\n    vocab_size=15475,\n    model_type='unigram',  # You can use 'bpe' instead if preferred\n    character_coverage=1.0,\n    pad_id=0, unk_id=1, bos_id=2, eos_id=3  # Consistent special token IDs\n)\n\n# Train English SentencePiece model\nspm.SentencePieceTrainer.train(\n    input='english_sentences.txt',\n    model_prefix='spm_english',\n    vocab_size=6897,\n    model_type='unigram',\n    character_coverage=1.0,\n    pad_id=0, unk_id=1, bos_id=2, eos_id=3\n)\n\n# Load the SentencePiece models\narabic_sp = spm.SentencePieceProcessor(model_file='spm_arabic.model')\nenglish_sp = spm.SentencePieceProcessor(model_file='spm_english.model')\n\n# Test tokenization\narabic_example = \"عمرك رايح المكسيك؟\"\nenglish_example = \"Have you ever been to Mexico?\"\nprint(\"Arabic Tokens:\", arabic_sp.encode(arabic_example, out_type=str))\nprint(\"Arabic IDs:\", arabic_sp.encode(arabic_example, out_type=int))\nprint(\"English Tokens:\", english_sp.encode(english_example, out_type=str))\nprint(\"English IDs:\", english_sp.encode(english_example, out_type=int))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T23:22:35.473578Z","iopub.execute_input":"2025-03-01T23:22:35.473896Z","iopub.status.idle":"2025-03-01T23:22:36.540881Z","shell.execute_reply.started":"2025-03-01T23:22:35.473872Z","shell.execute_reply":"2025-03-01T23:22:36.540114Z"}},"outputs":[{"name":"stdout","text":"Arabic Tokens: ['▁عمرك', '▁رايح', '▁المكسيك', '؟']\nArabic IDs: [3663, 1257, 4414, 7]\nEnglish Tokens: ['▁Have', '▁you', '▁ever', '▁been', '▁to', '▁Mexico', '?']\nEnglish IDs: [256, 12, 363, 111, 8, 2854, 13]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"def preprocess_sequence(sp_processor, sentence, max_len, bos_id, eos_id, pad_id):\n    \"\"\"Preprocess a sentence with SentencePiece.\"\"\"\n    tokens = sp_processor.encode(sentence, out_type=int)\n    tokens = [bos_id] + tokens + [eos_id]  # Add <BOS> and <EOS>\n    return pad_sequence(tokens, max_len, pad_id)\n\ndef pad_sequence(tokens, max_len, pad_id):\n    \"\"\"Pad or truncate sequence to max_len.\"\"\"\n    return tokens[:max_len] + [pad_id] * max(0, max_len - len(tokens)) if len(tokens) < max_len else tokens[:max_len]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T23:22:36.542263Z","iopub.execute_input":"2025-03-01T23:22:36.542492Z","iopub.status.idle":"2025-03-01T23:22:36.547332Z","shell.execute_reply.started":"2025-03-01T23:22:36.542474Z","shell.execute_reply":"2025-03-01T23:22:36.546499Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_len = 30\narabic_pad_id = 0  # From SentencePiece training\nenglish_sos_id = 2  # BOS\nenglish_eos_id = 3  # EOS\nenglish_pad_id = 0  # PAD\n\n# Tokenize and preprocess sequences\narabic_sequences = [pad_sequence(arabic_sp.encode(sentence, out_type=int), max_len, arabic_pad_id) \n                   for sentence in data_cleaned['Arabic']]\nenglish_sequences = [preprocess_sequence(english_sp, sentence, max_len, english_sos_id, english_eos_id, english_pad_id) \n                    for sentence in data_cleaned['English']]\n\n# Verify a sample\nprint(\"Sample Arabic Sequence:\", arabic_sequences[0])\nprint(\"Sample English Sequence:\", english_sequences[0])\nprint(\"Decoded English Sample:\", english_sp.decode(english_sequences[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T23:22:36.548574Z","iopub.execute_input":"2025-03-01T23:22:36.548804Z","iopub.status.idle":"2025-03-01T23:22:37.016626Z","shell.execute_reply.started":"2025-03-01T23:22:36.548785Z","shell.execute_reply":"2025-03-01T23:22:37.015968Z"}},"outputs":[{"name":"stdout","text":"Sample Arabic Sequence: [5, 2336, 2084, 2893, 5596, 2357, 11629, 31, 35, 2181, 1331, 717, 5, 5462, 5, 14170, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nSample English Sequence: [2, 185, 3732, 92, 301, 16, 123, 47, 46, 160, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nDecoded English Sample: If wisely used, money can do much.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n\n# Arabic BPE tokenizer setup\narabic_tokenizer = Tokenizer(models.BPE())\narabic_pre_tokenizer = pre_tokenizers.Whitespace()\narabic_tokenizer.pre_tokenizer = arabic_pre_tokenizer\narabic_trainer = trainers.BpeTrainer(vocab_size=20000, special_tokens=[\"<PAD>\", \"<UNK>\", \"<SOS>\", \"<EOS>\"])\n\n# Train the Arabic tokenizer\narabic_tokenizer.train(files=[\"arabic_sentences.txt\"], trainer=arabic_trainer)\narabic_tokenizer.save(\"arabic_bpe_tokenizer.json\")\n\n# English BPE tokenizer setup\nenglish_tokenizer = Tokenizer(models.BPE())\nenglish_pre_tokenizer = pre_tokenizers.Whitespace()\nenglish_tokenizer.pre_tokenizer = english_pre_tokenizer\nenglish_trainer = trainers.BpeTrainer(vocab_size=10000, special_tokens=[\"<PAD>\", \"<UNK>\", \"<SOS>\", \"<EOS>\"])\n\n# Train the English tokenizer\nenglish_tokenizer.train(files=[\"english_sentences.txt\"], trainer=english_trainer)\nenglish_tokenizer.save(\"english_bpe_tokenizer.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T23:22:39.419044Z","iopub.execute_input":"2025-03-01T23:22:39.419363Z","iopub.status.idle":"2025-03-01T23:22:40.390002Z","shell.execute_reply.started":"2025-03-01T23:22:39.419336Z","shell.execute_reply":"2025-03-01T23:22:40.389073Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Replace Cell 17\nimport torch\narabic_tensors = torch.tensor(arabic_sequences, dtype=torch.long)\nenglish_tensors = torch.tensor(english_sequences, dtype=torch.long)\nprint(\"Arabic Tensor Shape:\", arabic_tensors.shape)\nprint(\"English Tensor Shape:\", english_tensors.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T23:22:40.391435Z","iopub.execute_input":"2025-03-01T23:22:40.391694Z","iopub.status.idle":"2025-03-01T23:22:43.585366Z","shell.execute_reply.started":"2025-03-01T23:22:40.391673Z","shell.execute_reply":"2025-03-01T23:22:43.584391Z"}},"outputs":[{"name":"stdout","text":"Arabic Tensor Shape: torch.Size([19529, 30])\nEnglish Tensor Shape: torch.Size([19529, 30])\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\nclass TranslationDataset(Dataset):\n    def __init__(self, src_tensors, tgt_tensors):\n        self.src_tensors = src_tensors\n        self.tgt_tensors = tgt_tensors\n \n    def __len__(self):\n        return len(self.src_tensors)\n \n    def __getitem__(self, idx):\n        return self.src_tensors[idx], self.tgt_tensors[idx]\n\n# Create dataset\ntrain_dataset = TranslationDataset(arabic_tensors, english_tensors)\n\n# Create DataLoader for batching\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T23:22:43.586695Z","iopub.execute_input":"2025-03-01T23:22:43.586925Z","iopub.status.idle":"2025-03-01T23:22:43.592953Z","shell.execute_reply.started":"2025-03-01T23:22:43.586905Z","shell.execute_reply":"2025-03-01T23:22:43.592256Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nimport math\nimport copy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T23:22:45.714748Z","iopub.execute_input":"2025-03-01T23:22:45.715110Z","iopub.status.idle":"2025-03-01T23:22:45.720195Z","shell.execute_reply.started":"2025-03-01T23:22:45.715078Z","shell.execute_reply":"2025-03-01T23:22:45.719199Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# Transformer","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        # Ensure that the model dimension (d_model) is divisible by the number of heads\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n        \n        # Initialize dimensions\n        self.d_model = d_model # Model's dimension\n        self.num_heads = num_heads # Number of attention heads\n        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n        \n        # Linear layers for transforming inputs\n        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n        \n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        # Calculate attention scores\n        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        \n        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n        if mask is not None:\n            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n        \n        # Softmax is applied to obtain attention probabilities\n        attn_probs = torch.softmax(attn_scores, dim=-1)\n        \n        # Multiply by values to obtain the final output\n        output = torch.matmul(attn_probs, V)\n        return output\n        \n    def split_heads(self, x):\n        # Reshape the input to have num_heads for multi-head attention\n        batch_size, seq_length, d_model = x.size()\n        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n        \n    def combine_heads(self, x):\n        # Combine the multiple heads back to original shape\n        batch_size, _, seq_length, d_k = x.size()\n        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n        \n    def forward(self, Q, K, V, mask=None):\n        # Apply linear transformations and split heads\n        Q = self.split_heads(self.W_q(Q))\n        K = self.split_heads(self.W_k(K))\n        V = self.split_heads(self.W_v(V))\n        \n        # Perform scaled dot-product attention\n        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n        \n        # Combine heads and apply output transformation\n        output = self.W_o(self.combine_heads(attn_output))\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T23:22:46.093871Z","iopub.execute_input":"2025-03-01T23:22:46.094201Z","iopub.status.idle":"2025-03-01T23:22:46.102043Z","shell.execute_reply.started":"2025-03-01T23:22:46.094176Z","shell.execute_reply":"2025-03-01T23:22:46.101281Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"class PositionWiseFeedForward(nn.Module):\n    def __init__(self, d_model, d_ff):\n        super(PositionWiseFeedForward, self).__init__()\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        return self.fc2(self.relu(self.fc1(x)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T23:22:46.285410Z","iopub.execute_input":"2025-03-01T23:22:46.285684Z","iopub.status.idle":"2025-03-01T23:22:46.290343Z","shell.execute_reply.started":"2025-03-01T23:22:46.285663Z","shell.execute_reply":"2025-03-01T23:22:46.289397Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_seq_length):\n        super(PositionalEncoding, self).__init__()\n        \n        pe = torch.zeros(max_seq_length, d_model)\n        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        self.register_buffer('pe', pe.unsqueeze(0))\n        \n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T23:22:47.930167Z","iopub.execute_input":"2025-03-01T23:22:47.930463Z","iopub.status.idle":"2025-03-01T23:22:47.936132Z","shell.execute_reply.started":"2025-03-01T23:22:47.930442Z","shell.execute_reply":"2025-03-01T23:22:47.935227Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super(EncoderLayer, self).__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, mask):\n        attn_output = self.self_attn(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        ff_output = self.feed_forward(x)\n        x = self.norm2(x + self.dropout(ff_output))\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T23:22:48.148483Z","iopub.execute_input":"2025-03-01T23:22:48.148766Z","iopub.status.idle":"2025-03-01T23:22:48.154047Z","shell.execute_reply.started":"2025-03-01T23:22:48.148742Z","shell.execute_reply":"2025-03-01T23:22:48.153299Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"class DecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super(DecoderLayer, self).__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, enc_output, src_mask, tgt_mask):\n        attn_output = self.self_attn(x, x, x, tgt_mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n        x = self.norm2(x + self.dropout(attn_output))\n        ff_output = self.feed_forward(x)\n        x = self.norm3(x + self.dropout(ff_output))\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T23:22:48.369913Z","iopub.execute_input":"2025-03-01T23:22:48.370235Z","iopub.status.idle":"2025-03-01T23:22:48.376032Z","shell.execute_reply.started":"2025-03-01T23:22:48.370210Z","shell.execute_reply":"2025-03-01T23:22:48.375066Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n        super(Transformer, self).__init__()\n        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n\n        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n\n        self.fc = nn.Linear(d_model, tgt_vocab_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def generate_mask(self, src, tgt):\n        src_mask = (src != 0).unsqueeze(1).unsqueeze(2).to(src.device)\n        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3).to(tgt.device)\n        seq_length = tgt.size(1)\n        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length, device=tgt.device), diagonal=1)).bool()\n        # print(f\"src_mask device: {src_mask.device}, tgt_mask device: {tgt_mask.device}, nopeak_mask device: {nopeak_mask.device}\")\n        tgt_mask = tgt_mask & nopeak_mask\n        return src_mask, tgt_mask\n\n    def forward(self, src, tgt):\n        src_mask, tgt_mask = self.generate_mask(src, tgt)\n        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n\n        enc_output = src_embedded\n        for enc_layer in self.encoder_layers:\n            enc_output = enc_layer(enc_output, src_mask)\n\n        dec_output = tgt_embedded\n        for dec_layer in self.decoder_layers:\n            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n\n        output = self.fc(dec_output)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T23:22:48.583222Z","iopub.execute_input":"2025-03-01T23:22:48.583545Z","iopub.status.idle":"2025-03-01T23:22:48.591384Z","shell.execute_reply.started":"2025-03-01T23:22:48.583524Z","shell.execute_reply":"2025-03-01T23:22:48.590520Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"# Training ","metadata":{}},{"cell_type":"code","source":"# Hyperparameters\nsrc_vocab_size = 15475  # Matches spm_arabic vocab\ntgt_vocab_size = 6897  # Matches spm_english vocab\nd_model = 512          # Embedding dimension\nnum_heads = 8          # Number of attention heads\nnum_layers = 6         # Number of encoder/decoder layers\nd_ff = 2048            # Feedforward dimension\nmax_seq_length = 30    # Matches your current setup\ndropout = 0.3          # Dropout rate\n\n# Initialize model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = Transformer(\n    src_vocab_size=src_vocab_size,\n    tgt_vocab_size=tgt_vocab_size,\n    d_model=d_model,\n    num_heads=num_heads,\n    num_layers=num_layers,\n    d_ff=d_ff,\n    max_seq_length=max_seq_length,\n    dropout=dropout\n).to(device)\n\nmodel = nn.DataParallel(model)\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T23:22:50.505213Z","iopub.execute_input":"2025-03-01T23:22:50.505506Z","iopub.status.idle":"2025-03-01T23:22:51.476149Z","shell.execute_reply.started":"2025-03-01T23:22:50.505484Z","shell.execute_reply":"2025-03-01T23:22:51.475426Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Updated Cell 27\nimport torch.optim as optim\nimport torch.nn as nn\n\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\ncriterion = nn.CrossEntropyLoss(ignore_index=english_pad_id)  # Use english_pad_id (0)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n\ndef train_epoch(model, dataloader, optimizer, criterion):\n    model.train()\n    total_loss = 0\n    total_correct = 0\n    total_tokens = 0\n    for src, tgt in dataloader:\n        src = src.to(device)\n        tgt = tgt.to(device)\n        optimizer.zero_grad()\n        tgt_input = tgt[:, :-1].to(device)\n        tgt_output = tgt[:, 1:].to(device)\n        output = model(src, tgt_input)\n        loss = criterion(output.reshape(-1, tgt_vocab_size), tgt_output.reshape(-1))\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        total_loss += loss.item()\n        predictions = output.argmax(dim=-1)\n        mask = (tgt_output != english_pad_id)\n        correct = (predictions == tgt_output) & mask\n        total_correct += correct.sum().item()\n        total_tokens += mask.sum().item()\n    avg_loss = total_loss / len(dataloader)\n    accuracy = total_correct / total_tokens if total_tokens > 0 else 0\n    return avg_loss, accuracy\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T23:22:51.477191Z","iopub.execute_input":"2025-03-01T23:22:51.477419Z","iopub.status.idle":"2025-03-01T23:22:53.356028Z","shell.execute_reply.started":"2025-03-01T23:22:51.477400Z","shell.execute_reply":"2025-03-01T23:22:53.355112Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"num_epochs = 30\nfor epoch in range(num_epochs):\n    avg_loss, accuracy = train_epoch(model, train_loader, optimizer, criterion)\n    scheduler.step(avg_loss)\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n\nmodel_path = \"/kaggle/working/transformer_full_updated.pth\"\ntorch.save(model.module if isinstance(model, nn.DataParallel) else model, model_path)\nprint(f\"Full model saved to {model_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T23:22:55.851314Z","iopub.execute_input":"2025-03-01T23:22:55.851830Z","iopub.status.idle":"2025-03-01T23:58:35.614258Z","shell.execute_reply.started":"2025-03-01T23:22:55.851801Z","shell.execute_reply":"2025-03-01T23:58:35.613476Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/30, Loss: 4.7043, Accuracy: 0.3186\nEpoch 2/30, Loss: 3.8395, Accuracy: 0.3925\nEpoch 3/30, Loss: 3.4523, Accuracy: 0.4313\nEpoch 4/30, Loss: 3.1524, Accuracy: 0.4642\nEpoch 5/30, Loss: 2.8921, Accuracy: 0.4944\nEpoch 6/30, Loss: 2.6602, Accuracy: 0.5223\nEpoch 7/30, Loss: 2.4472, Accuracy: 0.5496\nEpoch 8/30, Loss: 2.2491, Accuracy: 0.5745\nEpoch 9/30, Loss: 2.0604, Accuracy: 0.5998\nEpoch 10/30, Loss: 1.8895, Accuracy: 0.6225\nEpoch 11/30, Loss: 1.7183, Accuracy: 0.6485\nEpoch 12/30, Loss: 1.5622, Accuracy: 0.6718\nEpoch 13/30, Loss: 1.4167, Accuracy: 0.6945\nEpoch 14/30, Loss: 1.2800, Accuracy: 0.7182\nEpoch 15/30, Loss: 1.1509, Accuracy: 0.7390\nEpoch 16/30, Loss: 1.0306, Accuracy: 0.7623\nEpoch 17/30, Loss: 0.9221, Accuracy: 0.7833\nEpoch 18/30, Loss: 0.8191, Accuracy: 0.8038\nEpoch 19/30, Loss: 0.7284, Accuracy: 0.8225\nEpoch 20/30, Loss: 0.6465, Accuracy: 0.8410\nEpoch 21/30, Loss: 0.5706, Accuracy: 0.8574\nEpoch 22/30, Loss: 0.5119, Accuracy: 0.8703\nEpoch 23/30, Loss: 0.4539, Accuracy: 0.8839\nEpoch 24/30, Loss: 0.4093, Accuracy: 0.8939\nEpoch 25/30, Loss: 0.3688, Accuracy: 0.9037\nEpoch 26/30, Loss: 0.3355, Accuracy: 0.9120\nEpoch 27/30, Loss: 0.3075, Accuracy: 0.9185\nEpoch 28/30, Loss: 0.2844, Accuracy: 0.9234\nEpoch 29/30, Loss: 0.2645, Accuracy: 0.9283\nEpoch 30/30, Loss: 0.2438, Accuracy: 0.9335\nFull model saved to /kaggle/working/transformer_full_updated.pth\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"def translate_sentence(model, arabic_sentence, arabic_sp, english_sp, max_len, device):\n    \"\"\"\n    Translate an Arabic sentence to English using the trained Transformer model.\n    \n    Args:\n        model: Trained Transformer model\n        arabic_sentence: String containing the Arabic input sentence\n        arabic_sp: SentencePiece processor for Arabic\n        english_sp: SentencePiece processor for English\n        max_len: Maximum sequence length (22 in your case)\n        device: torch.device (cuda or cpu)\n    Returns:\n        translated_sentence: String containing the English translation\n    \"\"\"\n    # Preprocess the input Arabic sentence\n    arabic_sentence = remove_diacritics(arabic_sentence)\n    \n    # Tokenize and encode the Arabic sentence\n    src = pad_sequence(arabic_sp.encode(arabic_sentence, out_type=int), max_len, 0)\n    src = torch.tensor([src], dtype=torch.long).to(device)\n    \n    # Initialize target sequence with <BOS> token\n    tgt = torch.tensor([[2]], dtype=torch.long).to(device)  # BOS = 2\n    \n    # Generate translation token by token\n    model.eval()\n    with torch.no_grad():\n        for _ in range(max_len - 1):\n            output = model(src, tgt)\n            next_token = output[:, -1, :].argmax(dim=-1).item()\n            tgt = torch.cat([tgt, torch.tensor([[next_token]], dtype=torch.long).to(device)], dim=1)\n            if next_token == 3:  # EOS = 3\n                break\n    \n    # Convert token IDs back to text\n    translated_ids = tgt[0].cpu().tolist()\n    if translated_ids[0] == 2:  # Remove BOS\n        translated_ids = translated_ids[1:]\n    if translated_ids[-1] == 3:  # Remove EOS\n        translated_ids = translated_ids[:-1]\n    \n    translated_sentence = english_sp.decode(translated_ids)\n    return translated_sentence\n\n# Example usage\narabic_test_sentence = \"أبي اعتاد أن يكون رجلاً قوياً.\"\ntranslated = translate_sentence(model, arabic_test_sentence, arabic_sp, english_sp, max_len=30, device=device)\nprint(f\"Arabic: {arabic_test_sentence}\")\nprint(f\"English: {translated}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T00:08:02.612870Z","iopub.execute_input":"2025-03-02T00:08:02.613246Z","iopub.status.idle":"2025-03-02T00:08:02.829375Z","shell.execute_reply.started":"2025-03-02T00:08:02.613221Z","shell.execute_reply":"2025-03-02T00:08:02.828474Z"}},"outputs":[{"name":"stdout","text":"Arabic: أبي اعتاد أن يكون رجلاً قوياً.\nEnglish: My father used to be a strong man.\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Sample English Sequence:\", english_sequences[10])\nprint(\"Contains <EOS>?\", 3 in english_sequences[10])  # Check if 3 (EOS) is in the sequence\nprint(\"Position of <EOS>:\", english_sequences[10].index(3) if 3 in english_sequences[10] else \"Not found\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T23:06:22.772466Z","iopub.execute_input":"2025-03-01T23:06:22.772763Z","iopub.status.idle":"2025-03-01T23:06:22.777845Z","shell.execute_reply.started":"2025-03-01T23:06:22.772740Z","shell.execute_reply":"2025-03-01T23:06:22.777170Z"}},"outputs":[{"name":"stdout","text":"Sample English Sequence: [2, 201, 6019, 73, 42, 1883, 25, 132, 7, 395, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nContains <EOS>? True\nPosition of <EOS>: 11\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"model = torch.load(\"/kaggle/input/newlytrained/transformer_full_updated (1).pth\")\nmodel.eval()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T22:25:18.999191Z","iopub.status.idle":"2025-03-01T22:25:18.999567Z","shell.execute_reply":"2025-03-01T22:25:18.999396Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}